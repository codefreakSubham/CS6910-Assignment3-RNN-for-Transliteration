{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Useful Imports\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport cv2\nimport pathlib\nimport tensorflow as tf\nimport tensorflow.keras as keras","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-27T18:10:38.761219Z","iopub.execute_input":"2022-04-27T18:10:38.761621Z","iopub.status.idle":"2022-04-27T18:10:47.124474Z","shell.execute_reply.started":"2022-04-27T18:10:38.761521Z","shell.execute_reply":"2022-04-27T18:10:47.123493Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Downloading the Dakshina dataset","metadata":{}},{"cell_type":"code","source":"#Downloading\n!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n    \n#Uncompressing\n!tar -xf dakshina_dataset_v1.0.tar","metadata":{"execution":{"iopub.status.busy":"2022-04-27T18:10:47.126181Z","iopub.execute_input":"2022-04-27T18:10:47.126509Z","iopub.status.idle":"2022-04-27T18:11:00.322305Z","shell.execute_reply.started":"2022-04-27T18:10:47.126472Z","shell.execute_reply":"2022-04-27T18:11:00.321079Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Pre processing data","metadata":{}},{"cell_type":"code","source":"def read(data_path, characters = False):\n    \n    # Returns the (x, y) pair from the dataset\n    # If characters == True, the input/output sample would be in the form list of characters, else as string\n\n    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n        lines = [line.split(\"\\t\") for line in f.read().split(\"\\n\") if line != '']\n    \n    x, y = [val[1] for val in lines], [val[0] for val in lines]\n    '''if characters:\n        input, target = [list(inp_str) for inp_str in input], [list(tar_str) for tar_str in target]'''\n    return x, y","metadata":{"execution":{"iopub.status.busy":"2022-04-27T18:11:00.324145Z","iopub.execute_input":"2022-04-27T18:11:00.324473Z","iopub.status.idle":"2022-04-27T18:11:00.332967Z","shell.execute_reply.started":"2022-04-27T18:11:00.324433Z","shell.execute_reply":"2022-04-27T18:11:00.331748Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"START_CHAR = '\\t'\nEND_CHAR = '\\n'\nBLANK_CHAR = ' '\ndef encode_decode_characters(train_input, train_target, val_input, val_target):\n    \n    # Returns the encoding for characters to integer (as a dictionary) and decoding for integers to characters (as a list) for input and target data\n    # Encoding and decoding of input vocabulary\n    \n    input_char_enc = {}\n    input_char_dec = []\n    max_encoder_seq_length = 1\n    for string in train_input + val_input:\n        max_encoder_seq_length = max(max_encoder_seq_length, len(string))\n        for char in string:\n            if char not in input_char_enc:\n                input_char_enc[char] = len(input_char_dec)\n                input_char_dec.append(char)\n    if BLANK_CHAR not in input_char_enc:\n        input_char_enc[BLANK_CHAR] = len(input_char_dec)\n        input_char_dec.append(BLANK_CHAR)\n        \n    # Encoding and decoding of target vocabulary\n    target_char_enc = {}\n    target_char_dec = []\n    target_char_enc[START_CHAR] = len(target_char_dec)\n    target_char_dec.append(START_CHAR)\n    max_decoder_seq_length = 1\n    for string in train_target + val_target:\n        max_decoder_seq_length = max(max_decoder_seq_length, len(string)+2)\n        for char in string:\n            if char not in target_char_enc:\n                target_char_enc[char] = len(target_char_dec)\n                target_char_dec.append(char)\n    target_char_enc[END_CHAR] = len(target_char_dec)\n    target_char_dec.append(END_CHAR)\n    if ' ' not in target_char_enc:\n        target_char_enc[BLANK_CHAR] = len(target_char_dec)\n        target_char_dec.append(BLANK_CHAR)\n\n    print(\"Number of training samples:\", len(train_input))\n    print(\"Number of validation samples:\", len(val_input))\n    print(\"Number of unique input tokens:\", len(input_char_dec))\n    print(\"Number of unique output tokens:\", len(target_char_dec))\n    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n\n    return input_char_enc, input_char_dec, target_char_enc, target_char_dec, max_encoder_seq_length, max_decoder_seq_length","metadata":{"execution":{"iopub.status.busy":"2022-04-27T18:11:00.335765Z","iopub.execute_input":"2022-04-27T18:11:00.336412Z","iopub.status.idle":"2022-04-27T18:11:01.613853Z","shell.execute_reply.started":"2022-04-27T18:11:00.336353Z","shell.execute_reply":"2022-04-27T18:11:01.612591Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def process_data(input, enc_timesteps, input_char_enc, target = None, dec_timesteps = None, target_char_enc = None):\n    \n    # Returns the input and target data in a form needed by the Keras embedding layer (i.e) \n    # decoder_input & encoder_input -- (None, timesteps) where each character is encoded by an integer\n    # decoder_output -- (None, timesteps, vocabulary size) where the last dimension is the one-hot encoding\n    # BLANK_CHAR -- space (equivalent to no meaningful input / blank input)\n    \n    encoder_input = np.array([[input_char_enc[ch] for ch in string] + [input_char_enc[BLANK_CHAR]] * (enc_timesteps - len(string)) for string in input])\n\n    decoder_input, decoder_target = None, None\n    if target is not None and dec_timesteps is not None and target_char_enc is not None:\n        \n        # START_CHAR -- start of sequence, END_CHAR -- end of sequence\n        decoder_input = np.array([[target_char_enc[START_CHAR]] + [target_char_enc[ch] for ch in string] + [target_char_enc[END_CHAR]] \n                                    + [target_char_enc[BLANK_CHAR]] * (dec_timesteps - len(string) - 2) for string in target])\n        decoder_target = np.zeros((decoder_input.shape[0], dec_timesteps, len(target_char_enc)), dtype='float32')\n\n        for i in range(decoder_input.shape[0]):\n            for t, char_ind in enumerate(decoder_input[i]):\n                if t > 0:\n                    decoder_target[i,t-1,char_ind] = 1.0\n            decoder_target[i,t:,target_char_enc[BLANK_CHAR]] = 1.0\n\n    return encoder_input, decoder_input, decoder_target","metadata":{"execution":{"iopub.status.busy":"2022-04-27T18:11:01.615027Z","iopub.execute_input":"2022-04-27T18:11:01.615348Z","iopub.status.idle":"2022-04-27T18:11:03.116066Z","shell.execute_reply.started":"2022-04-27T18:11:01.615311Z","shell.execute_reply":"2022-04-27T18:11:03.115377Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_x, train_y = read('./dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv')\nval_x, val_y = read('./dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv')\ntest_x, test_y = read('./dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv')\n\n# Assigning encoding and decoding for input and target characters\ninput_char_enc, input_char_dec, target_char_enc, target_char_dec, max_encoder_seq_length, max_decoder_seq_length = encode_decode_characters(\n    train_x, train_y, val_x, val_y)\n\n# Assigning training, validation and test encoder input, decoder input, decoder output\ntrain_enc_x, train_dec_x, train_dec_y = process_data(train_x, max_encoder_seq_length, input_char_enc, train_y, \n                                                                  max_decoder_seq_length, target_char_enc)\nval_enc_x, val_dec_x, val_dec_y = process_data(val_x, max_encoder_seq_length, input_char_enc, val_y, \n                                                            max_decoder_seq_length, target_char_enc)\ntest_enc_x, test_dec_x, test_dec_y = process_data(test_x, max_encoder_seq_length, input_char_enc, test_y, \n                                                               max_decoder_seq_length, target_char_enc)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T18:11:03.120260Z","iopub.execute_input":"2022-04-27T18:11:03.120527Z","iopub.status.idle":"2022-04-27T18:11:09.629926Z","shell.execute_reply.started":"2022-04-27T18:11:03.120497Z","shell.execute_reply":"2022-04-27T18:11:09.628870Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Seq2Seq Model (without Attention)","metadata":{}},{"cell_type":"markdown","source":"### Building the model","metadata":{}},{"cell_type":"code","source":"def build_model(encoder_vocab_size, decoder_vocab_size, inp_emb_size=64, n_enc_layers=1, n_dec_layers=1, \n                 h_layer_size=64, cell_type='RNN', dropout=0, r_dropout=0, cell_activation='tanh'):\n   \n    '''\n    Function to create a seq2seq model without attention.\n    Input :\n        encoder_vocab_size -- number of characters in input vocabulary (int)\n        decoder_vocab_size -- number of characters in output vocabulary (int)\n        inp_emb_size -- size of input embedding layer for encoder and decoder (int, default value : 64)\n        n_enc_layers -- number of layers of cell to stack in encoder (int, default value : 1)\n        n_dec_layers -- number of layers of cell to stack in decoder (int, default value : 1)\n        h_layer_size -- size of hidden layer of the encoder and decoder cells (int, default : 64)\n        cell_type -- type of cell used in encoder and decoder (string('LSTM'/ 'GRU'/ 'RNN'), default : 'LSTM')\n        dropout -- value of normal dropout (float(between 0 and 1), default : 0.0)\n        r_dropout -- value of recurrent dropout (float(between 0 and 1), default : 0.0)\n        cell_activation -- type of activation used in the cell (string, default : 'tanh')\n    Output :\n        model -- (Keras model object)\n    '''\n    \n    # Dictionary of different cell type\n    cell_dict = {\n        'RNN': keras.layers.SimpleRNN,\n        'GRU': keras.layers.GRU,\n        'LSTM': keras.layers.LSTM\n    }\n    \n    # Encoder input and embedding\n    encoder_input = keras.layers.Input(shape=(None,), name=\"input_1\")\n    encoder_inp_emb = keras.layers.Embedding(encoder_vocab_size, inp_emb_size, name=\"embedding_1\")(encoder_input)\n    \n    # Encoder cell layers\n    encoder_seq, *encoder_states = cell_dict[cell_type](h_layer_size, activation=cell_activation, return_sequences=True, return_state=True, \n                                                      dropout=dropout, recurrent_dropout=r_dropout, name=\"encoder_1\")(encoder_inp_emb)\n\n    for i in range(1, n_enc_layers):\n        encoder_seq, *encoder_states = cell_dict[cell_type](h_layer_size, activation=cell_activation, return_sequences=True, return_state=True, \n                                                          dropout=dropout, recurrent_dropout=r_dropout, name=\"encoder_\"+str(i+1))(encoder_seq)\n    \n    \n    # Decoder input and embedding\n    decoder_input = keras.layers.Input(shape=(None,), name=\"input_2\")\n    decoder_inp_emb = keras.layers.Embedding(decoder_vocab_size, inp_emb_size, name=\"embedding_2\")(decoder_input)\n\n    # Decoder cell layers\n    decoder_seq, *_ = cell_dict[cell_type](h_layer_size, activation=cell_activation, return_sequences=True, return_state=True, \n                                          dropout=dropout, recurrent_dropout=r_dropout, name=\"decoder_1\")(\n                                                decoder_inp_emb, initial_state=encoder_states\n                                         )\n    for i in range(1, n_dec_layers):\n        decoder_seq, *_ = cell_dict[cell_type](h_layer_size, activation=cell_activation, return_sequences=True, return_state=True, \n                                              dropout=dropout, recurrent_dropout=r_dropout, name=\"decoder_\"+str(i+1))(\n                                                    decoder_seq, initial_state=encoder_states\n                                             )\n    \n    # Softmax Fully Connected dense layer\n    decoder_dense_output = keras.layers.Dense(decoder_vocab_size, activation=\"softmax\", name=\"dense_1\")(\n        decoder_seq\n    )\n\n    # Finally the full encoder-decoder model\n    model = keras.Model([encoder_input, decoder_input], decoder_dense_output)\n\n    model.summary(line_length=200)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-27T18:11:09.631411Z","iopub.execute_input":"2022-04-27T18:11:09.631641Z","iopub.status.idle":"2022-04-27T18:11:09.656341Z","shell.execute_reply.started":"2022-04-27T18:11:09.631614Z","shell.execute_reply":"2022-04-27T18:11:09.655315Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"build_model(len(input_char_dec), len(target_char_dec))","metadata":{"execution":{"iopub.status.busy":"2022-04-27T18:11:09.658547Z","iopub.execute_input":"2022-04-27T18:11:09.658889Z","iopub.status.idle":"2022-04-27T18:11:10.180280Z","shell.execute_reply.started":"2022-04-27T18:11:09.658842Z","shell.execute_reply":"2022-04-27T18:11:10.179282Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Inference Model (without attention)","metadata":{}},{"cell_type":"code","source":"def create_inference_model(model):\n    '''\n    Function to return models needed for inference from the original model (without attention).\n    Arguments :\n        model -- (Keras model object) non-attention model used for training\n    Returns :\n        encoder_model -- (Keras model object) \n        deocder_model -- (Keras model object)\n        no_enc_layers -- (int) number of layers in the encoder\n        no_dec_layers -- (int) number of layers in the decoder\n    '''\n    # Calculating number of layers in encoder and decoder\n    no_enc_layers, no_dec_layers = 0, 0\n    for layer in model.layers:\n        no_enc_layers += layer.name.startswith('encoder')\n        no_dec_layers += layer.name.startswith('decoder')\n\n    # Encoder input\n    encoder_input = model.input[0]      # Input_1\n    # Encoder cell final layer\n    encoder_cell = model.get_layer(\"encoder_\"+str(no_enc_layers))\n    encoder_type = encoder_cell.__class__.__name__\n    encoder_seq, *encoder_state = encoder_cell.output\n    # Encoder model\n    encoder_model = keras.Model(encoder_input, encoder_state)\n\n    # Decoder input\n    decoder_input = model.input[1]      # Input_2\n    decoder_inp_emb = model.get_layer(\"embedding_2\")(decoder_input)\n    decoder_seq = decoder_inp_emb\n    # Inputs to decoder layers' initial states\n    decoder_states, decoder_state_inputs = [], []\n    for i in range(1, no_dec_layers+1):\n        if encoder_type == 'LSTM':\n            decoder_state_input = [keras.Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(2*i+1)), \n                                   keras.Input(shape=(encoder_state[1].shape[1],), name=\"input_\"+str(2*i+2))]\n        else:\n            decoder_state_input = [keras.Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(i+2))]\n\n        decoder_cell = model.get_layer(\"decoder_\"+str(i))\n        decoder_seq, *decoder_state = decoder_cell(decoder_seq, initial_state=decoder_state_input)\n        decoder_states += decoder_state\n        decoder_state_inputs += decoder_state_input\n\n    # Softmax FC layer\n    decoder_dense = model.get_layer(\"dense_1\")\n    decoder_dense_output = decoder_dense(decoder_seq)\n\n    # Decoder model\n    decoder_model = keras.Model(\n        [decoder_input] + decoder_state_inputs, [decoder_dense_output] + decoder_states\n    )\n\n    return encoder_model, decoder_model, no_enc_layers, no_dec_layers\n\n\ndef convert_to_word(predictions, char_enc, char_dec = None):\n    # Function to return the predictions after cutting the END_CHAR and BLANK_CHAR s at the end.\n    # If char_dec == None, the predictions are in the form of decoded string, otherwise as list of integers\n    no_samples = len(predictions) if type(predictions) is list else predictions.shape[0]\n    pred_words = ['' for _ in range(no_samples)]\n    for i, pred_list in enumerate(predictions):\n        for l in pred_list:\n            # Stop word : END_CHAR\n            if l == char_enc[END_CHAR]:\n                break\n            pred_words[i] += char_dec[l] if char_dec is not None else l\n    \n    return pred_words","metadata":{"execution":{"iopub.status.busy":"2022-04-27T18:24:02.308695Z","iopub.execute_input":"2022-04-27T18:24:02.310973Z","iopub.status.idle":"2022-04-27T18:24:02.332775Z","shell.execute_reply.started":"2022-04-27T18:24:02.310891Z","shell.execute_reply":"2022-04-27T18:24:02.331571Z"},"trusted":true},"execution_count":9,"outputs":[]}]}