{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Useful Imports\n\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport cv2\nimport pathlib\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.optimizers import Adam, SGD, RMSprop, Nadam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.utils.vis_utils import plot_model\nimport csv\nfrom IPython.display import HTML as html_print\nfrom IPython.display import display","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-07T06:15:13.422751Z","iopub.execute_input":"2022-05-07T06:15:13.423458Z","iopub.status.idle":"2022-05-07T06:15:19.282489Z","shell.execute_reply.started":"2022-05-07T06:15:13.423344Z","shell.execute_reply":"2022-05-07T06:15:19.281642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.exists('best_model.h5'):\n    os.remove('best_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:15:19.28437Z","iopub.execute_input":"2022-05-07T06:15:19.284633Z","iopub.status.idle":"2022-05-07T06:15:19.291822Z","shell.execute_reply.started":"2022-05-07T06:15:19.284596Z","shell.execute_reply":"2022-05-07T06:15:19.290522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Downloading the Dakshina dataset","metadata":{}},{"cell_type":"code","source":"#Downloading\n!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n    \n#Uncompressing\n!tar -xf dakshina_dataset_v1.0.tar","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:15:19.294443Z","iopub.execute_input":"2022-05-07T06:15:19.294787Z","iopub.status.idle":"2022-05-07T06:15:33.448886Z","shell.execute_reply.started":"2022-05-07T06:15:19.29475Z","shell.execute_reply":"2022-05-07T06:15:33.447988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pre processing data","metadata":{}},{"cell_type":"code","source":"def read(data_path, characters = False):\n    \n    # Returns the (x, y) pair from the dataset\n    # If characters == True, the input/output sample would be in the form list of characters, else as string\n\n    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n        lines = [line.split(\"\\t\") for line in f.read().split(\"\\n\") if line != '']\n    \n    x, y = [val[1] for val in lines], [val[0] for val in lines]\n    '''if characters:\n        input, target = [list(inp_str) for inp_str in input], [list(tar_str) for tar_str in target]'''\n    return x, y","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:15:33.45153Z","iopub.execute_input":"2022-05-07T06:15:33.451813Z","iopub.status.idle":"2022-05-07T06:15:33.461241Z","shell.execute_reply.started":"2022-05-07T06:15:33.45177Z","shell.execute_reply":"2022-05-07T06:15:33.46056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"START_CHAR = '\\t'\nEND_CHAR = '\\n'\nBLANK_CHAR = ' '\ndef encode_decode_characters(train_input, train_target, val_input, val_target):\n    \n    # Returns the encoding for characters to integer (as a dictionary) and decoding for integers to characters (as a list) for input and target data\n    # Encoding and decoding of input vocabulary\n    \n    input_char_enc = {}\n    input_char_dec = []\n    max_encoder_seq_length = 1\n    for string in train_input + val_input:\n        max_encoder_seq_length = max(max_encoder_seq_length, len(string))\n        for char in string:\n            if char not in input_char_enc:\n                input_char_enc[char] = len(input_char_dec)\n                input_char_dec.append(char)\n    if BLANK_CHAR not in input_char_enc:\n        input_char_enc[BLANK_CHAR] = len(input_char_dec)\n        input_char_dec.append(BLANK_CHAR)\n        \n    # Encoding and decoding of target vocabulary\n    target_char_enc = {}\n    target_char_dec = []\n    target_char_enc[START_CHAR] = len(target_char_dec)\n    target_char_dec.append(START_CHAR)\n    max_decoder_seq_length = 1\n    for string in train_target + val_target:\n        max_decoder_seq_length = max(max_decoder_seq_length, len(string)+2)\n        for char in string:\n            if char not in target_char_enc:\n                target_char_enc[char] = len(target_char_dec)\n                target_char_dec.append(char)\n    target_char_enc[END_CHAR] = len(target_char_dec)\n    target_char_dec.append(END_CHAR)\n    if ' ' not in target_char_enc:\n        target_char_enc[BLANK_CHAR] = len(target_char_dec)\n        target_char_dec.append(BLANK_CHAR)\n\n    print(\"Number of training samples:\", len(train_input))\n    print(\"Number of validation samples:\", len(val_input))\n    print(\"Number of unique input tokens:\", len(input_char_dec))\n    print(\"Number of unique output tokens:\", len(target_char_dec))\n    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n\n    return input_char_enc, input_char_dec, target_char_enc, target_char_dec, max_encoder_seq_length, max_decoder_seq_length","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:15:33.46239Z","iopub.execute_input":"2022-05-07T06:15:33.463368Z","iopub.status.idle":"2022-05-07T06:15:34.928806Z","shell.execute_reply.started":"2022-05-07T06:15:33.46334Z","shell.execute_reply":"2022-05-07T06:15:34.92796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_data(input, enc_timesteps, input_char_enc, target = None, dec_timesteps = None, target_char_enc = None):\n    \n    # Returns the input and target data in a form needed by the Keras embedding layer (i.e) \n    # decoder_input & encoder_input -- (None, timesteps) where each character is encoded by an integer\n    # decoder_output -- (None, timesteps, vocabulary size) where the last dimension is the one-hot encoding\n    # BLANK_CHAR -- space (equivalent to no meaningful input / blank input)\n    \n    encoder_input = np.array([[input_char_enc[ch] for ch in string] + [input_char_enc[BLANK_CHAR]] * (enc_timesteps - len(string)) for string in input])\n\n    decoder_input, decoder_target = None, None\n    if target is not None and dec_timesteps is not None and target_char_enc is not None:\n        \n        # START_CHAR -- start of sequence, END_CHAR -- end of sequence\n        decoder_input = np.array([[target_char_enc[START_CHAR]] + [target_char_enc[ch] for ch in string] + [target_char_enc[END_CHAR]] \n                                    + [target_char_enc[BLANK_CHAR]] * (dec_timesteps - len(string) - 2) for string in target])\n        decoder_target = np.zeros((decoder_input.shape[0], dec_timesteps, len(target_char_enc)), dtype='float32')\n\n        for i in range(decoder_input.shape[0]):\n            for t, char_ind in enumerate(decoder_input[i]):\n                if t > 0:\n                    decoder_target[i,t-1,char_ind] = 1.0\n            decoder_target[i,t:,target_char_enc[BLANK_CHAR]] = 1.0\n\n    return encoder_input, decoder_input, decoder_target","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:15:34.932019Z","iopub.execute_input":"2022-05-07T06:15:34.932533Z","iopub.status.idle":"2022-05-07T06:15:36.550686Z","shell.execute_reply.started":"2022-05-07T06:15:34.932502Z","shell.execute_reply":"2022-05-07T06:15:36.549921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x, train_y = read('./dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv')\nval_x, val_y = read('./dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv')\ntest_x, test_y = read('./dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv')\n\n# Assigning encoding and decoding for input and target characters\ninput_char_enc, input_char_dec, target_char_enc, target_char_dec, max_encoder_seq_length, max_decoder_seq_length = encode_decode_characters(\n    train_x, train_y, val_x, val_y)\n\n# Assigning training, validation and test encoder input, decoder input, decoder output\ntrain_enc_x, train_dec_x, train_dec_y = process_data(train_x, max_encoder_seq_length, input_char_enc, train_y, \n                                                                  max_decoder_seq_length, target_char_enc)\nval_enc_x, val_dec_x, val_dec_y = process_data(val_x, max_encoder_seq_length, input_char_enc, val_y, \n                                                            max_decoder_seq_length, target_char_enc)\ntest_enc_x, test_dec_x, test_dec_y = process_data(test_x, max_encoder_seq_length, input_char_enc, test_y, \n                                                               max_decoder_seq_length, target_char_enc)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:15:36.554325Z","iopub.execute_input":"2022-05-07T06:15:36.556061Z","iopub.status.idle":"2022-05-07T06:15:42.457022Z","shell.execute_reply.started":"2022-05-07T06:15:36.556018Z","shell.execute_reply":"2022-05-07T06:15:42.456272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Seq2Seq Model (without Attention)","metadata":{}},{"cell_type":"markdown","source":"### Building the model","metadata":{}},{"cell_type":"code","source":"def build_model(encoder_vocab_size, decoder_vocab_size, inp_emb_size=64, n_enc_layers=1, n_dec_layers=1, \n                 h_layer_size=64, cell_type='GRU', dropout=0, r_dropout=0, cell_activation='tanh'):\n   \n    '''\n    Function to create a seq2seq model without attention.\n    Input :\n        encoder_vocab_size -- number of characters in input vocabulary (int)\n        decoder_vocab_size -- number of characters in output vocabulary (int)\n        inp_emb_size -- size of input embedding layer for encoder and decoder (int, default value : 64)\n        n_enc_layers -- number of layers of cell to stack in encoder (int, default value : 1)\n        n_dec_layers -- number of layers of cell to stack in decoder (int, default value : 1)\n        h_layer_size -- size of hidden layer of the encoder and decoder cells (int, default : 64)\n        cell_type -- type of cell used in encoder and decoder (string('LSTM'/ 'GRU'/ 'RNN'), default : 'LSTM')\n        dropout -- value of normal dropout (float(between 0 and 1), default : 0.0)\n        r_dropout -- value of recurrent dropout (float(between 0 and 1), default : 0.0)\n        cell_activation -- type of activation used in the cell (string, default : 'tanh')\n    Output :\n        model -- (Keras model object)\n    '''\n    \n    # Dictionary of different cell type\n    cell_dict = {\n        'RNN': keras.layers.SimpleRNN,\n        'GRU': keras.layers.GRU,\n        'LSTM': keras.layers.LSTM\n    }\n    \n    # Encoder input and embedding\n    encoder_input = keras.layers.Input(shape=(None,), name=\"input_1\")\n    encoder_inp_emb = keras.layers.Embedding(encoder_vocab_size, inp_emb_size, name=\"embedding_1\")(encoder_input)\n    \n    # Encoder cell layers\n    encoder_seq, *encoder_states = cell_dict[cell_type](h_layer_size, activation=cell_activation, return_sequences=True, return_state=True, \n                                                      dropout=dropout, recurrent_dropout=r_dropout, name=\"encoder_1\")(encoder_inp_emb)\n\n    for i in range(1, n_enc_layers):\n        encoder_seq, *encoder_states = cell_dict[cell_type](h_layer_size, activation=cell_activation, return_sequences=True, return_state=True, \n                                                          dropout=dropout, recurrent_dropout=r_dropout, name=\"encoder_\"+str(i+1))(encoder_seq)\n    \n    \n    # Decoder input and embedding\n    decoder_input = keras.layers.Input(shape=(None,), name=\"input_2\")\n    decoder_inp_emb = keras.layers.Embedding(decoder_vocab_size, inp_emb_size, name=\"embedding_2\")(decoder_input)\n\n    # Decoder cell layers\n    decoder_seq, *_ = cell_dict[cell_type](h_layer_size, activation=cell_activation, return_sequences=True, return_state=True, \n                                          dropout=dropout, recurrent_dropout=r_dropout, name=\"decoder_1\")(\n                                                decoder_inp_emb, initial_state=encoder_states\n                                         )\n    for i in range(1, n_dec_layers):\n        decoder_seq, *_ = cell_dict[cell_type](h_layer_size, activation=cell_activation, return_sequences=True, return_state=True, \n                                              dropout=dropout, recurrent_dropout=r_dropout, name=\"decoder_\"+str(i+1))(\n                                                    decoder_seq, initial_state=encoder_states\n                                             )\n    \n    # Softmax Fully Connected dense layer\n    decoder_dense_output = keras.layers.Dense(decoder_vocab_size, activation=\"softmax\", name=\"dense_1\")(\n        decoder_seq\n    )\n\n    # Finally the full encoder-decoder model\n    model = keras.Model([encoder_input, decoder_input], decoder_dense_output)\n\n    #model.summary(line_length=200)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:15:42.460341Z","iopub.execute_input":"2022-05-07T06:15:42.460548Z","iopub.status.idle":"2022-05-07T06:15:42.475414Z","shell.execute_reply.started":"2022-05-07T06:15:42.460522Z","shell.execute_reply":"2022-05-07T06:15:42.474087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference Model (without attention)","metadata":{}},{"cell_type":"code","source":"def create_inference_model(model):\n    '''\n    Function to return models needed for inference from the original model (without attention).\n    Inputs :\n        model -- non-attention model used for training\n    Outputs :\n        encoder_model \n        deocder_model\n        no_enc_layers -- number of layers in the encoder(int)\n        no_dec_layers -- number of layers in the decoder(int)\n    '''\n    # Calculating number of layers in encoder and decoder\n    n_enc_layers, n_dec_layers = 0, 0\n    for layer in model.layers:\n        n_enc_layers += layer.name.startswith('encoder')\n        n_dec_layers += layer.name.startswith('decoder')\n\n    # Encoder input\n    encoder_input = model.input[0]      # Input_1\n    # Encoder cell final layer\n    encoder_cell = model.get_layer(\"encoder_\"+str(n_enc_layers))\n    encoder_type = encoder_cell.__class__.__name__\n    encoder_seq, *encoder_state = encoder_cell.output\n    # Encoder model\n    encoder_model = keras.Model(encoder_input, encoder_state)\n\n    # Decoder input\n    decoder_input = model.input[1]      # Input_2\n    decoder_inp_emb = model.get_layer(\"embedding_2\")(decoder_input)\n    decoder_seq = decoder_inp_emb\n    # Inputs to decoder layers' initial states\n    decoder_states, decoder_state_inputs = [], []\n    for i in range(1, n_dec_layers+1):\n        if encoder_type == 'LSTM':\n            decoder_state_input = [keras.Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(2*i+1)), \n                                   keras.Input(shape=(encoder_state[1].shape[1],), name=\"input_\"+str(2*i+2))]\n        else:\n            decoder_state_input = [keras.Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(i+2))]\n\n        decoder_cell = model.get_layer(\"decoder_\"+str(i))\n        decoder_seq, *decoder_state = decoder_cell(decoder_seq, initial_state=decoder_state_input)\n        decoder_states += decoder_state\n        decoder_state_inputs += decoder_state_input\n\n    # Softmax FC layer\n    decoder_dense = model.get_layer(\"dense_1\")\n    decoder_dense_output = decoder_dense(decoder_seq)\n\n    # Decoder model\n    decoder_model = keras.Model(\n        [decoder_input] + decoder_state_inputs, [decoder_dense_output] + decoder_states\n    )\n\n    return encoder_model, decoder_model, n_enc_layers, n_dec_layers\n\n\ndef convert_to_word(predictions, char_enc, char_dec = None):\n    \n    '''\n    Function to return the predictions after cutting the END_CHAR and BLANK_CHAR s at the end.\n    If char_dec == None, the predictions are in the form of decoded string, otherwise as list of integers\n    '''\n    \n    no_samples = len(predictions) if type(predictions) is list else predictions.shape[0]\n    pred_words = ['' for _ in range(no_samples)]\n    for i, pred_list in enumerate(predictions):\n        for l in pred_list:\n            # Stop word : END_CHAR\n            if l == char_enc[END_CHAR]:\n                break\n            pred_words[i] += char_dec[l] if char_dec is not None else l\n    \n    return pred_words","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:16:29.41055Z","iopub.execute_input":"2022-05-07T06:16:29.411178Z","iopub.status.idle":"2022-05-07T06:16:29.42465Z","shell.execute_reply.started":"2022-05-07T06:16:29.411144Z","shell.execute_reply":"2022-05-07T06:16:29.423952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Beam decoder","metadata":{}},{"cell_type":"code","source":"def beam_decoder_infer(model, input_seqs, max_decoder_timesteps, K=1, target_seqs=None, starting_char_enc=0, batch_size=64, attention=False):\n    \n    '''\n    Function to do inference on the model using beam decoder.\n    Inputs :\n        model -- training model\n        input_seqs -- input to encoder(numpy array, shape : (None, timesteps))\n        max_decoder_timesteps -- Number of timesteps to infer in decoder(int)\n        K -- beam width of beam decoder(int, default : 1)\n        target_seqs -- expected target(numpy array, shape : (None, timesteps, decoder_vocab_size), deault : None)\n                       If None, cross entropy errors won't be calculated.\n        starting_char_enc -- Encoding integer for START_CHAR(int, default : 0)\n        batch_size -- batch_size sent to Keras predict(int, default : 64)\n        attention -- whether the model has attention or not(bool, defualt : False)\n        \n    Outputs :\n        final_outputs -- top K output sequences(numpy array, shape : (None, K, timesteps))\n        final_errors -- cross entropy errors for top K output(numpy array, shape : (None, K))\n        states_values -- hidden states of decoder(numpy array, shape : (K, None, timesteps, hid_layer_size))\n        final_attn_scores -- attention to all encoder timesteps for a decoder timestep(numpy array, shape : (None, K, decoder_timesteps, encoder_timesteps))\n    '''\n    \n    # Generating output from encoder\n    encoder_model, decoder_model, no_enc_layers, no_dec_layers = create_attention_inference_model(model) if attention else create_inference_model(model)\n    encoder_output = encoder_model.predict(input_seqs, batch_size=batch_size)\n    encoder_out = encoder_output if type(encoder_output) is list else [encoder_output]\n\n    # Number of input samples in the data passed\n    no_samples = input_seqs.shape[0]\n\n    # Top K output sequences for each input \n    final_outputs = np.zeros((no_samples, K, max_decoder_timesteps), dtype=np.int32)\n    \n    # Errors for top K output sequences for each input\n    final_errors = np.zeros((no_samples, K))\n    \n    # Attention scores for top K output sequences for each input\n    final_attn_scores = np.zeros((no_samples, K, max_decoder_timesteps, input_seqs.shape[1]))\n\n    # decoder input sequence for 1 timestep (for all samples). Initially one choice only there\n    decoder_k_inputs = np.zeros((no_samples, 1, 1))\n    \n    # Populate the input sequence with the start character at the 1st timestep\n    decoder_k_inputs[:, :, 0] = starting_char_enc\n\n    # (log(probability) sequence, decoder output sequence) pairs for all choices and all samples. Probability starts with log(1) = 0\n    decoder_k_out = [[(0, [])] for _ in range(no_samples)]\n    \n    # Categorical cross entropy error in the sequence for all choice and all samples\n    errors = [[0] for _ in range(no_samples)]\n    \n    # Output states from decoder for all choices, and all samples\n    states_values  = [encoder_out * no_dec_layers]\n\n    # Attention weights output\n    attn_k_scores = [[None] for _ in range(no_samples)]\n\n    # Sampling loop\n    for it in range(max_decoder_timesteps):\n        # Storing respective data for all possibilities\n        All_k_beams = [[] for _ in range(no_samples)]\n        All_decoder_states = [[] for _ in range(no_samples)]\n        All_errors = [[] for _ in range(no_samples)]\n        All_attn_scores = [[] for _ in range(no_samples)]\n\n        for k in range(len(decoder_k_out[0])):\n            if attention:\n                attn_score, decoder_output, *decoder_states = decoder_model.predict([input_seqs, decoder_k_inputs[:,k]] + states_values[k], batch_size=batch_size)\n            else:\n                decoder_output, *decoder_states = decoder_model.predict([decoder_k_inputs[:,k]] + states_values[k], batch_size=batch_size)\n\n            # Top K scores\n            top_k = np.argsort(decoder_output[:, -1, :], axis=-1)[:, -K:]\n            for b in range(no_samples):\n                All_k_beams[b] += [(\n                    decoder_k_out[b][k][0] + np.log(decoder_output[b, -1, top_k[b][i]]),\n                    decoder_k_out[b][k][1] + [top_k[b][i]]\n                ) for i in range(K)]\n\n                if attention:\n                    All_attn_scores[b] += [attn_score[b]] * K if attn_k_scores[b][k] is None \\\n                                          else [np.concatenate((attn_k_scores[b][k], attn_score[b]), axis=0)] * K\n            \n                if target_seqs is not None:\n                    All_errors[b] += [errors[b][k] - np.log(decoder_output[b, -1, target_seqs[b, it]])] * K\n                \n                All_decoder_states[b] += [[state[b:b+1] for state in decoder_states]] * K\n        \n        # Sort and choose top K with max probabilities\n        sorted_ind = list(range(len(All_k_beams[0])))\n        sorted_ind = [sorted(sorted_ind, key = lambda ix: All_k_beams[b][ix][0])[-K:][::-1] for b in range(no_samples)]\n        \n        # Choose the top K decoder output sequences till now\n        decoder_k_out = [[All_k_beams[b][ind] for ind in sorted_ind[b]] for b in range(no_samples)]\n\n        # Update the input sequence for next 1 timestep\n        decoder_k_inputs = np.array([[All_k_beams[b][ind][1][-1] for ind in sorted_ind[b]] for b in range(no_samples)])\n\n        # Update states\n        states_values = [All_decoder_states[0][ind] for ind in sorted_ind[0]]\n        for b in range(1, no_samples):\n            states_values = [[np.concatenate((states_values[i][j], All_decoder_states[b][ind][j])) \n                              for j in range(len(All_decoder_states[b][ind]))] for i,ind in enumerate(sorted_ind[b])]\n\n        # Update attention scores\n        if attention:\n            attn_k_scores = [[All_attn_scores[b][ind] for ind in sorted_ind[b]] for b in range(no_samples)]\n\n        # Update errors\n        if target_seqs is not None:\n            errors = [[All_errors[b][ind] for ind in sorted_ind[b]] for b in range(no_samples)]\n\n    final_outputs = np.array([[decoder_k_out[b][i][1] for i in range(K)] for b in range(no_samples)])\n    if target_seqs is not None:\n        final_errors = np.array(errors) / max_decoder_timesteps\n    if attention:\n        final_attn_scores = np.array(attn_k_scores)\n\n    return final_outputs, final_errors, np.array(states_values), final_attn_scores\n\n\ndef calc_metrics(k_outputs, target_seqs, char_enc, char_dec, k_errors=None, exact_word=True):\n    \n    '''\n    Calculates the accuracy (and mean error if info provided) for the best of K possible output sequences\n    target_seqs -- Expected output (encoded sequence)\n    k_outputs -- k choices of output sequences for each sample\n    '''\n\n    matches = np.mean(k_outputs == np.repeat(target_seqs.reshape((target_seqs.shape[0], 1, target_seqs.shape[1])), k_outputs.shape[1], axis=1), axis=-1)\n    best_k = np.argmax(matches, axis=-1)\n    best_ind = (tuple(range(best_k.shape[0])), tuple(best_k))\n    accuracy = np.mean(matches[best_ind])\n\n    loss = None\n    if k_errors is not None:\n        loss = np.mean(k_errors[best_ind])\n    if exact_word:\n        equal = [0] * k_outputs.shape[0]\n        true_out = convert_to_word(target_seqs, char_enc, char_dec)\n        for k in range(k_outputs.shape[1]):\n            pred_out = convert_to_word(k_outputs[:,k], char_enc, char_dec)\n            equal = [equal[i] or (pred_out[i] == true_out[i]) for i in range(k_outputs.shape[0])]\n        exact_accuracy = np.mean(equal)\n\n        return accuracy, exact_accuracy, loss\n    \n    return accuracy, loss\n\n\ndef beam_decoder(model, input_seqs, target_seqs_onehot, max_decoder_timesteps, char_enc, char_dec, K=1, \n                 model_batch_size=64, attention=False, infer_batch_size=512, exact_word=True, return_outputs=False, \n                 return_states=False, return_attn_scores=False):\n    '''\n    Function to calculate/capture character-wise accuracy, exact-word-match accuracy, and loss for the seq2seq model using a beam decoder.\n    Inputs:\n        model -- model used for training\n        input_seqs -- input to encoder(numpy array, shape : (None, timesteps))\n        target_seqs -- expected target in onehot format(numpy array, shape : (None, timesteps, decoder_vocab_size))\n        max_decoder_timesteps -- Number of timesteps to infer in decoder(int)\n        char_enc -- target character encoding(dict)\n        char_dec -- target character decoding(list)\n        K -- beam width to be used in beam decoder(int, default : 1)\n        model_batch_size -- batch size to be used while evaluating model using Keras(int, default : 64)\n        attention -- whether the model has attention or not(bool, defualt : False)\n        infer_batch_size -- number of samples to be sent to beam_decoder_infer() at a time(int, default : 512)\n        exact_word -- whether or not exact_accuracy has(bool, default : True)\n        return_outputs -- whether or not the outputs predicted need to be returned(bool, default : True)\n        return_states -- whether or not the decoder hidden states need to be returned(bool, default : True)\n        return_attn_scores -- whether or not the attention scores need to be returned(bool, default : True)\n    Outputs:\n        accuracy -- the character-wise match accuracy(float)\n        (If exact_word is True) exact_accuracy -- (float) the exact word match accuracy\n        loss -- (float) the cross-entropy loss for the top K predictions\n        (If return_outputs is True) k_outputs -- (numpy ndarray of size : (None, K, timesteps)) top K output sequences\n        (If return_states is True) k_states -- (numpy ndarray of size : (K, None, timesteps, hid_layer_size))  hidden states of decoder\n        (If return_attn_scores is True) k_attn_scores -- (numpy ndarray of size : (None, K, decoder_timesteps, encoder_timesteps)) attention scores\n    '''\n    \n    target_seqs = np.argmax(target_seqs_onehot, axis=-1)\n    k_outputs, k_errors, k_states, k_attn_scores = None, None, None, None\n    for i in range(0, input_seqs.shape[0], infer_batch_size):\n        tmp_k_outputs, tmp_k_errors, tmp_k_states, tmp_k_attn_scores = beam_decoder_infer(model, input_seqs[i:i+infer_batch_size], \n                                                                                          max_decoder_timesteps, K, \n                                                                                          target_seqs[i:i+infer_batch_size], char_enc['\\t'], \n                                                                                          model_batch_size, attention)\n        if k_errors is None:\n            k_outputs, k_errors, k_states, k_attn_scores = tmp_k_outputs, tmp_k_errors, tmp_k_states, tmp_k_attn_scores\n        else:\n            k_outputs = np.concatenate((k_outputs, tmp_k_outputs))\n            k_errors = np.concatenate((k_errors, tmp_k_errors))\n            k_states = np.concatenate((k_states, tmp_k_states), axis=2)\n            k_attn_scores = np.concatenate((k_attn_scores, tmp_k_attn_scores))\n\n    return_elements = []\n    if return_outputs:\n        return_elements += [k_outputs]\n    if return_states:\n        return_elements += [k_states]\n    if return_attn_scores:\n        return_elements += [k_attn_scores]\n\n    if len(return_elements) > 0:\n        return calc_metrics(k_outputs, target_seqs, char_enc, char_dec, k_errors, exact_word) + tuple(return_elements)\n\n    return calc_metrics(k_outputs, target_seqs, char_enc, char_dec, k_errors, exact_word)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T07:44:50.335596Z","iopub.execute_input":"2022-05-07T07:44:50.33585Z","iopub.status.idle":"2022-05-07T07:44:50.380294Z","shell.execute_reply.started":"2022-05-07T07:44:50.335821Z","shell.execute_reply":"2022-05-07T07:44:50.379464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train function","metadata":{}},{"cell_type":"code","source":"def train(model, train_input_data, train_target_data, val_input_data, val_target_data, beam_width = 5, attention = False,\n                batch_size = 64, optimizer = 'adam', learning_rate = 0.0005, epochs = 15, loss_fn = 'categorical_crossentropy'):\n    \n    # Function to train the model using the mentioned optimizer, learning rate and epochs using given training and validation data\n\n    if optimizer == 'adam':\n        model.compile(optimizer = Adam(learning_rate=learning_rate), loss = loss_fn, metrics = ['accuracy'])\n    elif optimizer == 'momentum':\n        model.compile(optimizer = SGD(learning_rate=learning_rate, momentum = 0.9), loss = loss_fn, metrics = ['accuracy'])\n    elif optimizer == 'rmsprop':\n        model.compile(optimizer = RMSprop(learning_rate=learning_rate), loss = loss_fn, metrics = ['accuracy'])\n    elif optimizer == 'nesterov':\n        model.compile(optimizer = SGD(learning_rate=learning_rate, momentum = 0.9, nesterov = True), loss = loss_fn, metrics = ['accuracy'])\n    elif optimizer == 'nadam':\n        model.compile(optimizer = Nadam(learning_rate=learning_rate), loss = loss_fn, metrics = ['accuracy'])\n    else:\n        model.compile(optimizer = SGD(learning_rate=learning_rate), loss = loss_fn, metrics = ['accuracy'])\n\n    # Using validation accuracy as the metric\n    model.fit(train_input_data,\n              train_target_data,\n              batch_size = batch_size,\n              epochs = epochs\n             )\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:16:29.725053Z","iopub.execute_input":"2022-05-07T06:16:29.725413Z","iopub.status.idle":"2022-05-07T06:16:29.737211Z","shell.execute_reply.started":"2022-05-07T06:16:29.725374Z","shell.execute_reply":"2022-05-07T06:16:29.736404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function to calculate levenshtein distance between two sequences\nCode refered from: https://codereview.stackexchange.com/questions/217065/calculate-levenshtein-distance-between-two-strings-in-python","metadata":{}},{"cell_type":"code","source":"def levenshtein_dist(s1, s2):\n    \n    # Function to calculate levenshtein distance between two sequences\n    \"\"\"\n    The Levenshtein distance is a string metric for measuring the difference\n    between two sequences.\n    It is calculated as the minimum number of single-character edits necessary to\n    transform one string into another.\n    \"\"\"\n    \n    m, n = len(s1)+1, len(s2)+1\n    # Initialisation\n    dp = np.zeros((m, n))\n    # Base case\n    dp[0,1:] = np.arange(1,n)\n    dp[1:,0] = np.arange(1,m)\n\n    # Recursion\n    for i in range(1,m):\n        for j in range(1,n):\n            if s1[i-1] == s2[j-1]:\n                dp[i,j] = min(dp[i-1,j-1], dp[i-1,j]+1, dp[i,j-1]+1)\n            else:\n                dp[i,j] = min(dp[i,j-1], dp[i-1,j], dp[i-1,j-1]) + 1\n    \n    return dp[m-1,n-1]","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:16:29.739864Z","iopub.execute_input":"2022-05-07T06:16:29.74008Z","iopub.status.idle":"2022-05-07T06:16:29.750712Z","shell.execute_reply.started":"2022-05-07T06:16:29.740054Z","shell.execute_reply":"2022-05-07T06:16:29.749936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function to Test model","metadata":{}},{"cell_type":"code","source":"def test(model, test_enc_input, test_dec_target, max_decoder_seq_length, target_char_enc, target_char_dec, test_input=None):\n    '''\n    Function to evaluate the model metrics on test data and optionally save the predictions.\n    Inputs :\n        test_enc_input -- input to encoder(numpy array shape : (None, timesteps)) (where characters are encoded as integers)\n        test_dec_target -- expected target in onehot format(numpy array shape : (None, timesteps, decoder_vocab_size))\n        max_decoder_seq_length -- number of timesteps in the decoder(int)\n        target_enc_enc -- target character encoding(dict)\n        target_char_dec -- target character decoding(list)\n        test_input -- input as words (list of strings)\n    Outputs :\n        acc -- character-wise match accuracy(float)\n        exact_K_acc -- exact word match accuracy using the beam width for the model(float)\n        exact_acc -- exact word match accuracy using the first prediction (float)(which is equivalent to beam width = 1)\n        loss -- loss value(float)\n        true_out -- true output(list of string : (no_samples, word))\n        pred_out -- predicted output(2D list of string : (no_samples, K, word))\n        pred_scores -- levenshtein distance of prediction to true output(2D list : (no_samples, K))\n        model -- the model obtained from the run\n    '''\n    \n\n    no_samples, K, batch_size = test_enc_input.shape[0], 5, 64\n    acc, exact_K_acc, loss, outputs = beam_decoder(model, test_enc_input, test_dec_target, max_decoder_seq_length, target_char_enc, \n                                                                target_char_dec, K, batch_size, False,\n                                                                return_outputs=True, return_attn_scores=False)\n    \n    print(f'Test accuracy (word level using beam width = {K}) : {exact_K_acc*100:.2f}%')\n\n    test_target = np.argmax(test_dec_target, axis=-1)\n    true_out = convert_to_word(test_target, target_char_enc, target_char_dec)\n    pred_out = [[] for _ in range(no_samples)]\n    pred_scores = [[] for _ in range(no_samples)]\n    for k in range(K):\n        pred = convert_to_word(outputs[:,k], target_char_enc, target_char_dec)\n        pred_out = [pred_out[b] + [pred[b]] for b in range(no_samples)]\n        pred_scores = [pred_scores[b] + [levenshtein_dist(pred[b], true_out[b])] for b in range(no_samples)]\n    \n    equal = [pred_out[i][0] == true_out[i] for i in range(no_samples)]\n    exact_acc = np.mean(equal)\n\n    print(f'Test accuracy ((word level using first prediction) : {exact_acc*100:.2f}%')\n    print('\\n')\n    \n    # Writing top k predictions in CSV file\n    pred_file_name = 'predictions_vanilla.csv'\n    with open(pred_file_name, 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([\"Input\"] + [\"Prediction_\"+str(k+1) for k in range(K)])\n        for b in range(no_samples):\n            writer.writerow([test_input[b]] + [pred_out[b][k] for k in range(K)])\n\n    return acc, exact_K_acc, exact_acc, loss, true_out, pred_out, pred_scores, model","metadata":{"execution":{"iopub.status.busy":"2022-05-07T08:23:16.56473Z","iopub.execute_input":"2022-05-07T08:23:16.565028Z","iopub.status.idle":"2022-05-07T08:23:16.580437Z","shell.execute_reply.started":"2022-05-07T08:23:16.564997Z","shell.execute_reply":"2022-05-07T08:23:16.579392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Print sample predictions","metadata":{}},{"cell_type":"code","source":"def get_clr(value, cmap=None):\n    \n  # Function to get appropriate color for a value between 0 and 1 from the default blue to red hard-coded colors or a matplotlib cmap \n  \n  colors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8',\n    '#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n    '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n    '#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n  if cmap is not None:\n      rgba = matplotlib.cm.get_cmap(cmap)(value)\n      return 'rgb'+str(tuple([int(c*255) for c in rgba[:-1]]))\n  value = min(int((value * 100) / 5), 19)\n  return colors[value]\n\ndef visualize_samples(input, true_out, pred_out, pred_scores, rand_seq=None):\n    \n    '''\n    Function to print sample outputs in a neat format\n    Arguments :\n        input -- input words\n        true_out -- true output as words\n        pred_out -- K predicted output words\n        pred_scores -- levenshtein distance for the predictions to the true output\n        rand_seq -- list of indices from the dataset passed for which the sample outputs are to be printed (If None, random 10 samples will be chosen)\n    Returns :\n        rand_seq -- the list of indices for which sample outputs are printed\n    '''\n    \n    n_samples = len(true_out)\n    if rand_seq is None:\n        rand_seq = np.random.randint(n_samples, size=(10,))\n    print('-'*20 + f' Top {len(pred_scores[0])} predictions in decreasing order of probabilities for 10 random samples ' + '-'*20)\n    print('')\n    for i in rand_seq:\n        K = len(pred_scores[i])\n        html_str = '''\n        <table style=\"border:2px solid black; border-collapse:collapse\">\n        <caption> <strong>INPUT :</strong> {} &emsp; | &emsp; <strong> TRUE OUTPUT : </strong> {} </caption>\n        <tr>\n        <th scope=\"row\" style=\"border:1px solid black;padding:10px;text-align:left\"> Top {} Predictions </th>\n        '''.format(input[i], true_out[i], K)\n        for k in range(K):\n            html_str += '''\n            <td style=\"color:#000;background-color:{};border:1px solid black;padding:10px\"> {} </td>\n            '''.format(get_clr(pred_scores[i][k]/5), pred_out[i][k])\n        html_str += '''\n        </tr>\n        <tr>\n        <th scope=\"row\" style=\"border:1px solid black;padding:10px;text-align:left\"> Levenshtein distance (to true output) &emsp; </th>\n        '''\n        for k in range(K):\n            html_str += '''\n            <td style=\"border:1px solid black;padding:10px\"> {} </td>\n            '''.format(pred_scores[i][k])\n        html_str += '''\n        </tr>\n        </table>\n        '''\n        display(html_print(html_str))\n        print('\\n\\n')\n    \n    return rand_seq","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:16:29.775031Z","iopub.execute_input":"2022-05-07T06:16:29.775446Z","iopub.status.idle":"2022-05-07T06:16:29.78835Z","shell.execute_reply.started":"2022-05-07T06:16:29.775408Z","shell.execute_reply":"2022-05-07T06:16:29.787659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing with the best model","metadata":{}},{"cell_type":"code","source":"#Building the model with the best hyperparameters\nmodel = build_model(len(input_char_dec), len(target_char_dec), \n                    inp_emb_size=256, n_enc_layers=2, \n                    n_dec_layers=5, h_layer_size=256, \n                    cell_type='GRU', dropout=0.3, r_dropout=0.3)\n\n\n#Training the model with best set of hyperparameters\nmodel = train(model = model, train_input_data= [train_enc_x,train_dec_x], train_target_data= train_dec_y, \n                      val_input_data= [val_enc_x,val_dec_x], val_target_data= val_dec_y, beam_width= 5,\n                      attention = True, batch_size= 256, optimizer = 'adam', learning_rate= 0.001, \n                      epochs= 15)\nmodel.save(\"best_model_attn.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-05-07T06:16:29.83197Z","iopub.execute_input":"2022-05-07T06:16:29.832859Z","iopub.status.idle":"2022-05-07T07:12:09.32743Z","shell.execute_reply.started":"2022-05-07T06:16:29.832809Z","shell.execute_reply":"2022-05-07T07:12:09.326686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Testing the model with best set of hyperparameters\n\ntest_acc, test_exact_K_acc, test_exact_acc, test_loss, test_true_out,\\\ntest_pred_out, test_pred_scores, model =test(model, test_enc_x, \n                                                          test_dec_y, max_decoder_seq_length, \n                                                          target_char_enc, \n                                                          target_char_dec, test_x)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T08:23:26.171248Z","iopub.execute_input":"2022-05-07T08:23:26.171545Z","iopub.status.idle":"2022-05-07T08:31:26.300194Z","shell.execute_reply.started":"2022-05-07T08:23:26.171514Z","shell.execute_reply":"2022-05-07T08:31:26.299353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing random samples of predictions","metadata":{}},{"cell_type":"code","source":"random_samples = visualize_samples(test_x, test_true_out, test_pred_out, test_pred_scores)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T08:37:21.470876Z","iopub.execute_input":"2022-05-07T08:37:21.471544Z","iopub.status.idle":"2022-05-07T08:37:21.507043Z","shell.execute_reply.started":"2022-05-07T08:37:21.471501Z","shell.execute_reply":"2022-05-07T08:37:21.506217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualise the best model (with attention)","metadata":{}},{"cell_type":"code","source":"plot_model(model, to_file=\"model_vanilla.png\", show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-07T08:42:47.48425Z","iopub.execute_input":"2022-05-07T08:42:47.484558Z","iopub.status.idle":"2022-05-07T08:42:47.737598Z","shell.execute_reply.started":"2022-05-07T08:42:47.484523Z","shell.execute_reply":"2022-05-07T08:42:47.73678Z"},"trusted":true},"execution_count":null,"outputs":[]}]}