{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Useful Imports\n\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport cv2\nimport pathlib\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.optimizers import Adam, SGD, RMSprop, Nadam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.utils.vis_utils import plot_model\nimport csv\nfrom IPython.display import HTML as html_print\nfrom IPython.display import display\nimport wandb\nfrom wandb.keras import WandbCallback","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.exists('best_model.h5'):\n    os.remove('best_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Downloading the Dakshina dataset","metadata":{}},{"cell_type":"code","source":"#Downloading\n!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n    \n#Uncompressing\n!tar -xf dakshina_dataset_v1.0.tar","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pre processing data","metadata":{}},{"cell_type":"code","source":"def read(data_path, characters = False):\n    \n    # Returns the (x, y) pair from the dataset\n    # If characters == True, the input/output sample would be in the form list of characters, else as string\n\n    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n        lines = [line.split(\"\\t\") for line in f.read().split(\"\\n\") if line != '']\n    \n    x, y = [val[1] for val in lines], [val[0] for val in lines]\n    '''if characters:\n        input, target = [list(inp_str) for inp_str in input], [list(tar_str) for tar_str in target]'''\n    return x, y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"START_CHAR = '\\t'\nEND_CHAR = '\\n'\nBLANK_CHAR = ' '\ndef encode_decode_characters(train_input, train_target, val_input, val_target):\n    \n    # Returns the encoding for characters to integer (as a dictionary) and decoding for integers to characters (as a list) for input and target data\n    # Encoding and decoding of input vocabulary\n    \n    input_char_enc = {}\n    input_char_dec = []\n    max_encoder_seq_length = 1\n    for string in train_input + val_input:\n        max_encoder_seq_length = max(max_encoder_seq_length, len(string))\n        for char in string:\n            if char not in input_char_enc:\n                input_char_enc[char] = len(input_char_dec)\n                input_char_dec.append(char)\n    if BLANK_CHAR not in input_char_enc:\n        input_char_enc[BLANK_CHAR] = len(input_char_dec)\n        input_char_dec.append(BLANK_CHAR)\n        \n    # Encoding and decoding of target vocabulary\n    target_char_enc = {}\n    target_char_dec = []\n    target_char_enc[START_CHAR] = len(target_char_dec)\n    target_char_dec.append(START_CHAR)\n    max_decoder_seq_length = 1\n    for string in train_target + val_target:\n        max_decoder_seq_length = max(max_decoder_seq_length, len(string)+2)\n        for char in string:\n            if char not in target_char_enc:\n                target_char_enc[char] = len(target_char_dec)\n                target_char_dec.append(char)\n    target_char_enc[END_CHAR] = len(target_char_dec)\n    target_char_dec.append(END_CHAR)\n    if ' ' not in target_char_enc:\n        target_char_enc[BLANK_CHAR] = len(target_char_dec)\n        target_char_dec.append(BLANK_CHAR)\n\n    print(\"Number of training samples:\", len(train_input))\n    print(\"Number of validation samples:\", len(val_input))\n    print(\"Number of unique input tokens:\", len(input_char_dec))\n    print(\"Number of unique output tokens:\", len(target_char_dec))\n    print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n    print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n\n    return input_char_enc, input_char_dec, target_char_enc, target_char_dec, max_encoder_seq_length, max_decoder_seq_length","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_data(input, enc_timesteps, input_char_enc, target = None, dec_timesteps = None, target_char_enc = None):\n    \n    # Returns the input and target data in a form needed by the Keras embedding layer (i.e) \n    # decoder_input & encoder_input -- (None, timesteps) where each character is encoded by an integer\n    # decoder_output -- (None, timesteps, vocabulary size) where the last dimension is the one-hot encoding\n    # BLANK_CHAR -- space (equivalent to no meaningful input / blank input)\n    \n    encoder_input = np.array([[input_char_enc[ch] for ch in string] + [input_char_enc[BLANK_CHAR]] * (enc_timesteps - len(string)) for string in input])\n\n    decoder_input, decoder_target = None, None\n    if target is not None and dec_timesteps is not None and target_char_enc is not None:\n        \n        # START_CHAR -- start of sequence, END_CHAR -- end of sequence\n        decoder_input = np.array([[target_char_enc[START_CHAR]] + [target_char_enc[ch] for ch in string] + [target_char_enc[END_CHAR]] \n                                    + [target_char_enc[BLANK_CHAR]] * (dec_timesteps - len(string) - 2) for string in target])\n        decoder_target = np.zeros((decoder_input.shape[0], dec_timesteps, len(target_char_enc)), dtype='float32')\n\n        for i in range(decoder_input.shape[0]):\n            for t, char_ind in enumerate(decoder_input[i]):\n                if t > 0:\n                    decoder_target[i,t-1,char_ind] = 1.0\n            decoder_target[i,t:,target_char_enc[BLANK_CHAR]] = 1.0\n\n    return encoder_input, decoder_input, decoder_target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x, train_y = read('./dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.train.tsv')\nval_x, val_y = read('./dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.dev.tsv')\ntest_x, test_y = read('./dakshina_dataset_v1.0/bn/lexicons/bn.translit.sampled.test.tsv')\n\n# Assigning encoding and decoding for input and target characters\ninput_char_enc, input_char_dec, target_char_enc, target_char_dec, max_encoder_seq_length, max_decoder_seq_length = encode_decode_characters(\n    train_x, train_y, val_x, val_y)\n\n# Assigning training, validation and test encoder input, decoder input, decoder output\ntrain_enc_x, train_dec_x, train_dec_y = process_data(train_x, max_encoder_seq_length, input_char_enc, train_y, \n                                                                  max_decoder_seq_length, target_char_enc)\nval_enc_x, val_dec_x, val_dec_y = process_data(val_x, max_encoder_seq_length, input_char_enc, val_y, \n                                                            max_decoder_seq_length, target_char_enc)\ntest_enc_x, test_dec_x, test_dec_y = process_data(test_x, max_encoder_seq_length, input_char_enc, test_y, \n                                                               max_decoder_seq_length, target_char_enc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Seq2Seq Model (without Attention)","metadata":{}},{"cell_type":"markdown","source":"### Building the model","metadata":{}},{"cell_type":"code","source":"def build_model(encoder_vocab_size, decoder_vocab_size, inp_emb_size=64, n_enc_layers=1, n_dec_layers=1, \n                 h_layer_size=64, cell_type='GRU', dropout=0, r_dropout=0, cell_activation='tanh'):\n   \n    '''\n    Function to create a seq2seq model without attention.\n    Input :\n        encoder_vocab_size -- number of characters in input vocabulary (int)\n        decoder_vocab_size -- number of characters in output vocabulary (int)\n        inp_emb_size -- size of input embedding layer for encoder and decoder (int, default value : 64)\n        n_enc_layers -- number of layers of cell to stack in encoder (int, default value : 1)\n        n_dec_layers -- number of layers of cell to stack in decoder (int, default value : 1)\n        h_layer_size -- size of hidden layer of the encoder and decoder cells (int, default : 64)\n        cell_type -- type of cell used in encoder and decoder (string('LSTM'/ 'GRU'/ 'RNN'), default : 'LSTM')\n        dropout -- value of normal dropout (float(between 0 and 1), default : 0.0)\n        r_dropout -- value of recurrent dropout (float(between 0 and 1), default : 0.0)\n        cell_activation -- type of activation used in the cell (string, default : 'tanh')\n    Output :\n        model -- (Keras model object)\n    '''\n    \n    # Dictionary of different cell type\n    cell_dict = {\n        'RNN': keras.layers.SimpleRNN,\n        'GRU': keras.layers.GRU,\n        'LSTM': keras.layers.LSTM\n    }\n    \n    # Encoder input and embedding\n    encoder_input = keras.layers.Input(shape=(None,), name=\"input_1\")\n    encoder_inp_emb = keras.layers.Embedding(encoder_vocab_size, inp_emb_size, name=\"embedding_1\")(encoder_input)\n    \n    # Encoder cell layers\n    encoder_seq, *encoder_states = cell_dict[cell_type](h_layer_size, activation=cell_activation, return_sequences=True, return_state=True, \n                                                      dropout=dropout, recurrent_dropout=r_dropout, name=\"encoder_1\")(encoder_inp_emb)\n\n    for i in range(1, n_enc_layers):\n        encoder_seq, *encoder_states = cell_dict[cell_type](h_layer_size, activation=cell_activation, return_sequences=True, return_state=True, \n                                                          dropout=dropout, recurrent_dropout=r_dropout, name=\"encoder_\"+str(i+1))(encoder_seq)\n    \n    \n    # Decoder input and embedding\n    decoder_input = keras.layers.Input(shape=(None,), name=\"input_2\")\n    decoder_inp_emb = keras.layers.Embedding(decoder_vocab_size, inp_emb_size, name=\"embedding_2\")(decoder_input)\n\n    # Decoder cell layers\n    decoder_seq, *_ = cell_dict[cell_type](h_layer_size, activation=cell_activation, return_sequences=True, return_state=True, \n                                          dropout=dropout, recurrent_dropout=r_dropout, name=\"decoder_1\")(\n                                                decoder_inp_emb, initial_state=encoder_states\n                                         )\n    for i in range(1, n_dec_layers):\n        decoder_seq, *_ = cell_dict[cell_type](h_layer_size, activation=cell_activation, return_sequences=True, return_state=True, \n                                              dropout=dropout, recurrent_dropout=r_dropout, name=\"decoder_\"+str(i+1))(\n                                                    decoder_seq, initial_state=encoder_states\n                                             )\n    \n    # Softmax Fully Connected dense layer\n    decoder_dense_output = keras.layers.Dense(decoder_vocab_size, activation=\"softmax\", name=\"dense_1\")(\n        decoder_seq\n    )\n\n    # Finally the full encoder-decoder model\n    model = keras.Model([encoder_input, decoder_input], decoder_dense_output)\n\n    #model.summary(line_length=200)\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference Model (without attention)","metadata":{}},{"cell_type":"code","source":"def create_inference_model(model):\n    '''\n    Function to return models needed for inference from the original model (without attention).\n    Inputs :\n        model -- non-attention model used for training\n    Outputs :\n        encoder_model \n        deocder_model\n        no_enc_layers -- number of layers in the encoder(int)\n        no_dec_layers -- number of layers in the decoder(int)\n    '''\n    # Calculating number of layers in encoder and decoder\n    n_enc_layers, n_dec_layers = 0, 0\n    for layer in model.layers:\n        n_enc_layers += layer.name.startswith('encoder')\n        n_dec_layers += layer.name.startswith('decoder')\n\n    # Encoder input\n    encoder_input = model.input[0]      # Input_1\n    # Encoder cell final layer\n    encoder_cell = model.get_layer(\"encoder_\"+str(n_enc_layers))\n    encoder_type = encoder_cell.__class__.__name__\n    encoder_seq, *encoder_state = encoder_cell.output\n    # Encoder model\n    encoder_model = keras.Model(encoder_input, encoder_state)\n\n    # Decoder input\n    decoder_input = model.input[1]      # Input_2\n    decoder_inp_emb = model.get_layer(\"embedding_2\")(decoder_input)\n    decoder_seq = decoder_inp_emb\n    # Inputs to decoder layers' initial states\n    decoder_states, decoder_state_inputs = [], []\n    for i in range(1, n_dec_layers+1):\n        if encoder_type == 'LSTM':\n            decoder_state_input = [keras.Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(2*i+1)), \n                                   keras.Input(shape=(encoder_state[1].shape[1],), name=\"input_\"+str(2*i+2))]\n        else:\n            decoder_state_input = [keras.Input(shape=(encoder_state[0].shape[1],), name=\"input_\"+str(i+2))]\n\n        decoder_cell = model.get_layer(\"decoder_\"+str(i))\n        decoder_seq, *decoder_state = decoder_cell(decoder_seq, initial_state=decoder_state_input)\n        decoder_states += decoder_state\n        decoder_state_inputs += decoder_state_input\n\n    # Softmax FC layer\n    decoder_dense = model.get_layer(\"dense_1\")\n    decoder_dense_output = decoder_dense(decoder_seq)\n\n    # Decoder model\n    decoder_model = keras.Model(\n        [decoder_input] + decoder_state_inputs, [decoder_dense_output] + decoder_states\n    )\n\n    return encoder_model, decoder_model, n_enc_layers, n_dec_layers\n\n\ndef convert_to_word(predictions, char_enc, char_dec = None):\n    \n    '''\n    Function to return the predictions after cutting the END_CHAR and BLANK_CHAR s at the end.\n    If char_dec == None, the predictions are in the form of decoded string, otherwise as list of integers\n    '''\n    \n    no_samples = len(predictions) if type(predictions) is list else predictions.shape[0]\n    pred_words = ['' for _ in range(no_samples)]\n    for i, pred_list in enumerate(predictions):\n        for l in pred_list:\n            # Stop word : END_CHAR\n            if l == char_enc[END_CHAR]:\n                break\n            pred_words[i] += char_dec[l] if char_dec is not None else l\n    \n    return pred_words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Beam decoder","metadata":{}},{"cell_type":"code","source":"def beam_decoder_infer(model, input_seqs, max_decoder_timesteps, K=1, target_seqs=None, starting_char_enc=0, batch_size=64, attention=False):\n    \n    '''\n    Function to do inference on the model using beam decoder.\n    Inputs :\n        model -- training model\n        input_seqs -- input to encoder(numpy array, shape : (None, timesteps))\n        max_decoder_timesteps -- Number of timesteps to infer in decoder(int)\n        K -- beam width of beam decoder(int, default : 1)\n        target_seqs -- expected target(numpy array, shape : (None, timesteps, decoder_vocab_size), deault : None)\n                       If None, cross entropy errors won't be calculated.\n        starting_char_enc -- Encoding integer for START_CHAR(int, default : 0)\n        batch_size -- batch_size sent to Keras predict(int, default : 64)\n        attention -- whether the model has attention or not(bool, defualt : False)\n        \n    Outputs :\n        final_outputs -- top K output sequences(numpy array, shape : (None, K, timesteps))\n        final_errors -- cross entropy errors for top K output(numpy array, shape : (None, K))\n        states_values -- hidden states of decoder(numpy array, shape : (K, None, timesteps, hid_layer_size))\n        final_attn_scores -- attention to all encoder timesteps for a decoder timestep(numpy array, shape : (None, K, decoder_timesteps, encoder_timesteps))\n    '''\n    \n    # Generating output from encoder\n    encoder_model, decoder_model, no_enc_layers, no_dec_layers = create_attention_inference_model(model) if attention else create_inference_model(model)\n    encoder_output = encoder_model.predict(input_seqs, batch_size=batch_size)\n    encoder_out = encoder_output if type(encoder_output) is list else [encoder_output]\n\n    # Number of input samples in the data passed\n    no_samples = input_seqs.shape[0]\n\n    # Top K output sequences for each input \n    final_outputs = np.zeros((no_samples, K, max_decoder_timesteps), dtype=np.int32)\n    \n    # Errors for top K output sequences for each input\n    final_errors = np.zeros((no_samples, K))\n    \n    # Attention scores for top K output sequences for each input\n    final_attn_scores = np.zeros((no_samples, K, max_decoder_timesteps, input_seqs.shape[1]))\n\n    # decoder input sequence for 1 timestep (for all samples). Initially one choice only there\n    decoder_k_inputs = np.zeros((no_samples, 1, 1))\n    \n    # Populate the input sequence with the start character at the 1st timestep\n    decoder_k_inputs[:, :, 0] = starting_char_enc\n\n    # (log(probability) sequence, decoder output sequence) pairs for all choices and all samples. Probability starts with log(1) = 0\n    decoder_k_out = [[(0, [])] for _ in range(no_samples)]\n    \n    # Categorical cross entropy error in the sequence for all choice and all samples\n    errors = [[0] for _ in range(no_samples)]\n    \n    # Output states from decoder for all choices, and all samples\n    states_values  = [encoder_out * no_dec_layers]\n\n    # Attention weights output\n    attn_k_scores = [[None] for _ in range(no_samples)]\n\n    # Sampling loop\n    for it in range(max_decoder_timesteps):\n        # Storing respective data for all possibilities\n        All_k_beams = [[] for _ in range(no_samples)]\n        All_decoder_states = [[] for _ in range(no_samples)]\n        All_errors = [[] for _ in range(no_samples)]\n        All_attn_scores = [[] for _ in range(no_samples)]\n\n        for k in range(len(decoder_k_out[0])):\n            if attention:\n                attn_score, decoder_output, *decoder_states = decoder_model.predict([input_seqs, decoder_k_inputs[:,k]] + states_values[k], batch_size=batch_size)\n            else:\n                decoder_output, *decoder_states = decoder_model.predict([decoder_k_inputs[:,k]] + states_values[k], batch_size=batch_size)\n\n            # Top K scores\n            top_k = np.argsort(decoder_output[:, -1, :], axis=-1)[:, -K:]\n            for b in range(no_samples):\n                All_k_beams[b] += [(\n                    decoder_k_out[b][k][0] + np.log(decoder_output[b, -1, top_k[b][i]]),\n                    decoder_k_out[b][k][1] + [top_k[b][i]]\n                ) for i in range(K)]\n\n                if attention:\n                    All_attn_scores[b] += [attn_score[b]] * K if attn_k_scores[b][k] is None \\\n                                          else [np.concatenate((attn_k_scores[b][k], attn_score[b]), axis=0)] * K\n            \n                if target_seqs is not None:\n                    All_errors[b] += [errors[b][k] - np.log(decoder_output[b, -1, target_seqs[b, it]])] * K\n                \n                All_decoder_states[b] += [[state[b:b+1] for state in decoder_states]] * K\n        \n        # Sort and choose top K with max probabilities\n        sorted_ind = list(range(len(All_k_beams[0])))\n        sorted_ind = [sorted(sorted_ind, key = lambda ix: All_k_beams[b][ix][0])[-K:][::-1] for b in range(no_samples)]\n        \n        # Choose the top K decoder output sequences till now\n        decoder_k_out = [[All_k_beams[b][ind] for ind in sorted_ind[b]] for b in range(no_samples)]\n\n        # Update the input sequence for next 1 timestep\n        decoder_k_inputs = np.array([[All_k_beams[b][ind][1][-1] for ind in sorted_ind[b]] for b in range(no_samples)])\n\n        # Update states\n        states_values = [All_decoder_states[0][ind] for ind in sorted_ind[0]]\n        for b in range(1, no_samples):\n            states_values = [[np.concatenate((states_values[i][j], All_decoder_states[b][ind][j])) \n                              for j in range(len(All_decoder_states[b][ind]))] for i,ind in enumerate(sorted_ind[b])]\n\n        # Update attention scores\n        if attention:\n            attn_k_scores = [[All_attn_scores[b][ind] for ind in sorted_ind[b]] for b in range(no_samples)]\n\n        # Update errors\n        if target_seqs is not None:\n            errors = [[All_errors[b][ind] for ind in sorted_ind[b]] for b in range(no_samples)]\n\n    final_outputs = np.array([[decoder_k_out[b][i][1] for i in range(K)] for b in range(no_samples)])\n    if target_seqs is not None:\n        final_errors = np.array(errors) / max_decoder_timesteps\n    if attention:\n        final_attn_scores = np.array(attn_k_scores)\n\n    return final_outputs, final_errors, np.array(states_values), final_attn_scores\n\n\ndef calc_metrics(k_outputs, target_seqs, char_enc, char_dec, k_errors=None, exact_word=True):\n    \n    '''\n    Calculates the accuracy (and mean error if info provided) for the best of K possible output sequences\n    target_seqs -- Expected output (encoded sequence)\n    k_outputs -- k choices of output sequences for each sample\n    '''\n\n    matches = np.mean(k_outputs == np.repeat(target_seqs.reshape((target_seqs.shape[0], 1, target_seqs.shape[1])), k_outputs.shape[1], axis=1), axis=-1)\n    best_k = np.argmax(matches, axis=-1)\n    best_ind = (tuple(range(best_k.shape[0])), tuple(best_k))\n    accuracy = np.mean(matches[best_ind])\n\n    loss = None\n    if k_errors is not None:\n        loss = np.mean(k_errors[best_ind])\n    if exact_word:\n        equal = [0] * k_outputs.shape[0]\n        true_out = convert_to_word(target_seqs, char_enc, char_dec)\n        for k in range(k_outputs.shape[1]):\n            pred_out = convert_to_word(k_outputs[:,k], char_enc, char_dec)\n            equal = [equal[i] or (pred_out[i] == true_out[i]) for i in range(k_outputs.shape[0])]\n        exact_accuracy = np.mean(equal)\n\n        return accuracy, exact_accuracy, loss\n    \n    return accuracy, loss\n\n\ndef beam_decoder(model, input_seqs, target_seqs_onehot, max_decoder_timesteps, char_enc, char_dec, K=1, \n                 model_batch_size=64, attention=False, infer_batch_size=512, exact_word=True, return_outputs=False, \n                 return_states=False, return_attn_scores=False):\n    '''\n    Function to calculate/capture character-wise accuracy, exact-word-match accuracy, and loss for the seq2seq model using a beam decoder.\n    Inputs:\n        model -- model used for training\n        input_seqs -- input to encoder(numpy array, shape : (None, timesteps))\n        target_seqs -- expected target in onehot format(numpy array, shape : (None, timesteps, decoder_vocab_size))\n        max_decoder_timesteps -- Number of timesteps to infer in decoder(int)\n        char_enc -- target character encoding(dict)\n        char_dec -- target character decoding(list)\n        K -- beam width to be used in beam decoder(int, default : 1)\n        model_batch_size -- batch size to be used while evaluating model using Keras(int, default : 64)\n        attention -- whether the model has attention or not(bool, defualt : False)\n        infer_batch_size -- number of samples to be sent to beam_decoder_infer() at a time(int, default : 512)\n        exact_word -- whether or not exact_accuracy has(bool, default : True)\n        return_outputs -- whether or not the outputs predicted need to be returned(bool, default : True)\n        return_states -- whether or not the decoder hidden states need to be returned(bool, default : True)\n        return_attn_scores -- whether or not the attention scores need to be returned(bool, default : True)\n    Outputs:\n        accuracy -- the character-wise match accuracy(float)\n        (If exact_word is True) exact_accuracy -- (float) the exact word match accuracy\n        loss -- (float) the cross-entropy loss for the top K predictions\n        (If return_outputs is True) k_outputs -- (numpy ndarray of size : (None, K, timesteps)) top K output sequences\n        (If return_states is True) k_states -- (numpy ndarray of size : (K, None, timesteps, hid_layer_size))  hidden states of decoder\n        (If return_attn_scores is True) k_attn_scores -- (numpy ndarray of size : (None, K, decoder_timesteps, encoder_timesteps)) attention scores\n    '''\n    \n    target_seqs = np.argmax(target_seqs_onehot, axis=-1)\n    k_outputs, k_errors, k_states, k_attn_scores = None, None, None, None\n    for i in range(0, input_seqs.shape[0], infer_batch_size):\n        tmp_k_outputs, tmp_k_errors, tmp_k_states, tmp_k_attn_scores = beam_decoder_infer(model, input_seqs[i:i+infer_batch_size], \n                                                                                          max_decoder_timesteps, K, \n                                                                                          target_seqs[i:i+infer_batch_size], char_enc['\\t'], \n                                                                                          model_batch_size, attention)\n        if k_errors is None:\n            k_outputs, k_errors, k_states, k_attn_scores = tmp_k_outputs, tmp_k_errors, tmp_k_states, tmp_k_attn_scores\n        else:\n            k_outputs = np.concatenate((k_outputs, tmp_k_outputs))\n            k_errors = np.concatenate((k_errors, tmp_k_errors))\n            k_states = np.concatenate((k_states, tmp_k_states), axis=2)\n            k_attn_scores = np.concatenate((k_attn_scores, tmp_k_attn_scores))\n\n    return_elements = []\n    if return_outputs:\n        return_elements += [k_outputs]\n    if return_states:\n        return_elements += [k_states]\n    if return_attn_scores:\n        return_elements += [k_attn_scores]\n\n    if len(return_elements) > 0:\n        return calc_metrics(k_outputs, target_seqs, char_enc, char_dec, k_errors, exact_word) + tuple(return_elements)\n\n    return calc_metrics(k_outputs, target_seqs, char_enc, char_dec, k_errors, exact_word)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class customValidation(keras.callbacks.Callback):\n    # Custom class to provide callback after each epoch of training to calculate custom metrics for validation set with beam decoder\n    def __init__(self, val_enc_input, val_dec_target, beam_width=1, batch_size=64, attention=False):\n        self.beam_width = beam_width\n        self.validation_input = val_enc_input\n        self.validation_target = val_dec_target\n        self.batch_size = batch_size\n        self.attention = attention\n\n    def on_epoch_end(self, epoch, logs):\n        val_accuracy, val_exact_accuracy, val_loss = beam_decoder(self.model, self.validation_input, self.validation_target, max_decoder_seq_length, \n                                                                  target_char_enc, target_char_dec, self.beam_width, self.batch_size, self.attention)\n\n        # Log them to reflect in WANDB callback and EarlyStopping\n        logs[\"val_accuracy\"] = val_accuracy\n        logs[\"val_exact_accuracy\"] = val_exact_accuracy\n        logs[\"val_loss\"] = val_loss             # Validation loss calculates categorical cross entropy loss\n\n        print(\"— val_loss: {:.4f} — val_accuracy: {:.4f} — val_exact_accuracy: {:.4f}\".format(val_loss, val_accuracy, val_exact_accuracy))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train function","metadata":{}},{"cell_type":"code","source":"def train(model, train_input_data, train_target_data, val_input_data, val_target_data, beam_width = 5, attention = False,\n                batch_size = 64, optimizer = 'adam', learning_rate = 0.0005, epochs = 15, loss_fn = 'categorical_crossentropy'):\n    \n    # Function to train the model using the mentioned optimizer, learning rate and epochs using given training and validation data\n\n    if optimizer == 'adam':\n        model.compile(optimizer = Adam(learning_rate=learning_rate), loss = loss_fn, metrics = ['accuracy'])\n    elif optimizer == 'momentum':\n        model.compile(optimizer = SGD(learning_rate=learning_rate, momentum = 0.9), loss = loss_fn, metrics = ['accuracy'])\n    elif optimizer == 'rmsprop':\n        model.compile(optimizer = RMSprop(learning_rate=learning_rate), loss = loss_fn, metrics = ['accuracy'])\n    elif optimizer == 'nesterov':\n        model.compile(optimizer = SGD(learning_rate=learning_rate, momentum = 0.9, nesterov = True), loss = loss_fn, metrics = ['accuracy'])\n    elif optimizer == 'nadam':\n        model.compile(optimizer = Nadam(learning_rate=learning_rate), loss = loss_fn, metrics = ['accuracy'])\n    else:\n        model.compile(optimizer = SGD(learning_rate=learning_rate), loss = loss_fn, metrics = ['accuracy'])\n\n    # Using validation accuracy as the metric\n    model.fit(train_input_data,\n              train_target_data,\n              batch_size = batch_size,\n              epochs = epochs\n              callbacks = [customValidation(val_input_data[0], val_target_data, beam_width, batch_size, attention), \n                           WandbCallback(monitor='val_accuracy'), EarlyStopping(monitor='val_accuracy', patience=5)]\n             )\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### WandB Login\n","metadata":{}},{"cell_type":"code","source":"#Change with appropriate login key to login successfully\n!wandb login 48af1d62ac54f77717fbb3680bc61c553ce36124","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sweep wrapper function for running sweeps","metadata":{}},{"cell_type":"code","source":"def wandb_wrapper():\n    # Wrapper function for trainig the model with different hyperparamerters\n    \n    wandb.init(project=\"Seq2SeqLearning\", entity=\"cs21s048-cs21s058\")\n\n    config = wandb.config\n\n    wandb.run.name = f'ie_{config.inp_emb_size}_ne_{config.n_enc_layers}_de_{config.n_dec_layers}_ct_{config.cell_type}_dr_{config.dropout}'\n    wandb.run.name += f'_da_{config.h_layer_size}_K_{config.beam_width}'\n    wandb.run.save()\n    print(wandb.run.name)\n    \n    model = build_model(len(input_char_dec), len(target_char_dec), config.inp_emb_size, config.n_enc_layers, \n                         config.n_dec_layers, config.h_layer_size, config.cell_type, config.dropout, config.dropout)\n    \n    model = train(model = model, train_input_data= [train_enc_x,train_dec_x], train_target_data= train_dec_y, \n                      val_input_data= [val_enc_x,val_dec_x], val_target_data= val_dec_y, beam_width= config.beam_width,\n                      attention = False, batch_size= config.batch_size, optimizer = 'adam', learning_rate= config.learning_rate, \n                      epochs= config.epochs)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setting up wandb sweeps","metadata":{}},{"cell_type":"code","source":"sweep_config = {\n    'name': 'Seq2SeQ_without_Attention',\n    'method': 'bayes',            \n    'metric': {\n      'name': 'val_accuracy',\n      'goal': 'maximize'   \n    },\n    'parameters': {\n        'epochs': {\n            'values': [10, 15]\n        },\n        'batch_size': {\n            'values': [128, 256]\n        },\n        'inp_emb_size': {\n            'values': [256, 512]\n        },\n        'n_enc_layers': {\n            'values': [1, 2]\n        },\n        'n_dec_layers': {\n            'values': [3, 5]\n        },\n        'h_layer_size': {\n            'values': [256, 512, 768]\n        },\n        'cell_type': {\n            'values': ['RNN', 'LSTM', 'GRU']\n        },\n        'dropout' :{\n            'values': [0, 0.3]\n        },\n        'beam_width': {\n            'values': [1, 3, 5]\n        },\n        'learning_rate': {\n            'values': [0.001, 0.0001, 0.0005]\n        }\n    }\n}\n\n#creating the sweep (change with appropriate project and entity name befor running sweep)\nsweep_id = wandb.sweep(sweep_config, project=\"Seq2SeqLearning\", entity=\"cs21s048-cs21s058\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Run sweep\nwandb.agent(sweep_id, function=wandb_wrapper, count=100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}