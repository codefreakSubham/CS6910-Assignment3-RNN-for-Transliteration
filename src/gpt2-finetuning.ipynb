{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GPT 2 Fine tuning for generating sentences\n\n### Refered from the following blog post:\nhttps://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272\n\n**Dataset used:** https://www.kaggle.com/datasets/neisse/scrapped-lyrics-from-6-genres","metadata":{}},{"cell_type":"code","source":"# Useful Imports\n\nimport pandas as pd\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport numpy as np\nimport random\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm, trange\nimport torch.nn.functional as F\nimport csv\nfrom csv import reader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-04T17:41:17.681744Z","iopub.execute_input":"2022-05-04T17:41:17.681992Z","iopub.status.idle":"2022-05-04T17:41:24.00846Z","shell.execute_reply.started":"2022-05-04T17:41:17.681921Z","shell.execute_reply":"2022-05-04T17:41:24.00772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning raw data","metadata":{}},{"cell_type":"code","source":"### Prepare data\nlyrics = pd.read_csv('../input/scrapped-lyrics-from-6-genres/lyrics-data.csv')\n\n#Only keep popular artists, with genre Rock/Pop and popularity high enough\nartists = pd.read_csv('../input/scrapped-lyrics-from-6-genres/artists-data.csv')\nartists = artists[(artists['Genres'].isin(['Rock'])) & (artists['Popularity']>5)]\ndf = lyrics.merge(artists[['Artist', 'Genres', 'Link']], left_on='ALink', right_on='Link', how='inner')\ndf = df.drop(columns=['ALink','SLink','Link'])\n\n#Drop the songs with lyrics too long (after more than 1024 tokens, does not work)\ndf = df[df['Lyric'].apply(lambda x: len(x.split(' ')) < 350)]\n\n#Create a very small test set to compare generated text with the reality\ntest_set = df.sample(n = 200)\ndf = df.loc[~df.index.isin(test_set.index)]\n\n#Reset the indexes\ntest_set = test_set.reset_index()\ndf = df.reset_index()\n\n#For the test set only, keep last 20 words in a new column, then remove them from original column\ntest_set['True_end_lyrics'] = test_set['Lyric'].str.split().str[-20:].apply(' '.join)\ntest_set['Lyric'] = test_set['Lyric'].str.split().str[:-20].apply(' '.join)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T17:41:24.010067Z","iopub.execute_input":"2022-05-04T17:41:24.010337Z","iopub.status.idle":"2022-05-04T17:41:33.255548Z","shell.execute_reply.started":"2022-05-04T17:41:24.010302Z","shell.execute_reply":"2022-05-04T17:41:33.254827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating the dataset for training","metadata":{}},{"cell_type":"code","source":"class SongLyrics(Dataset):  \n    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n        self.lyrics = []\n\n        for row in df['Lyric']:\n          self.lyrics.append(torch.tensor(\n                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n            ))               \n        if truncate:\n            self.lyrics = self.lyrics[:20000]\n        self.lyrics_count = len(self.lyrics)\n        \n    def __len__(self):\n        return self.lyrics_count\n\n    def __getitem__(self, item):\n        return self.lyrics[item]\n    \ndataset = SongLyrics(df['Lyric'], truncate=True, gpt2_type=\"gpt2\") ","metadata":{"execution":{"iopub.status.busy":"2022-05-04T17:41:33.256908Z","iopub.execute_input":"2022-05-04T17:41:33.257158Z","iopub.status.idle":"2022-05-04T17:41:47.308665Z","shell.execute_reply.started":"2022-05-04T17:41:33.257125Z","shell.execute_reply":"2022-05-04T17:41:47.307989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tockenizing the data to give input to GPT Model","metadata":{}},{"cell_type":"code","source":"#Get the tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n#Accumulated batch size (since GPT2 is so big)\ndef pack_tensor(new_tensor, packed_tensor, max_seq_len):\n    if packed_tensor is None:\n        return new_tensor, True, None\n    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n        return packed_tensor, False, new_tensor\n    else:\n        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n        return packed_tensor, True, None","metadata":{"execution":{"iopub.status.busy":"2022-05-04T17:41:47.31063Z","iopub.execute_input":"2022-05-04T17:41:47.310887Z","iopub.status.idle":"2022-05-04T17:42:31.278882Z","shell.execute_reply.started":"2022-05-04T17:41:47.310852Z","shell.execute_reply":"2022-05-04T17:42:31.278145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### The train function","metadata":{}},{"cell_type":"code","source":"def train(\n    dataset, model, tokenizer,\n    batch_size=16, epochs=5, lr=2e-5,\n    max_seq_len=400, warmup_steps=200,\n    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n    test_mode=False,save_model_on_epoch=False,\n):\n    acc_steps = 100\n    device=torch.device(\"cuda\")\n    model = model.cuda()\n    model.train()\n\n    optimizer = AdamW(model.parameters(), lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n    )\n\n    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n    loss=0\n    accumulating_batch_count = 0\n    input_tensor = None\n\n    for epoch in range(epochs):\n\n        print(f\"Training epoch {epoch}\")\n        print(loss)\n        for idx, entry in tqdm(enumerate(train_dataloader)):\n            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n\n            if carry_on and idx != len(train_dataloader) - 1:\n                continue\n\n            input_tensor = input_tensor.to(device)\n            outputs = model(input_tensor, labels=input_tensor)\n            loss = outputs[0]\n            loss.backward()\n\n            if (accumulating_batch_count % batch_size) == 0:\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n                model.zero_grad()\n\n            accumulating_batch_count += 1\n            input_tensor = None\n        if save_model_on_epoch:\n            torch.save(\n                model.state_dict(),\n                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n            )\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-05-04T17:42:31.280447Z","iopub.execute_input":"2022-05-04T17:42:31.280932Z","iopub.status.idle":"2022-05-04T17:42:31.368678Z","shell.execute_reply.started":"2022-05-04T17:42:31.280894Z","shell.execute_reply":"2022-05-04T17:42:31.368016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training the model","metadata":{}},{"cell_type":"code","source":"model = train(dataset, model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2022-05-04T17:42:31.369602Z","iopub.execute_input":"2022-05-04T17:42:31.369799Z","iopub.status.idle":"2022-05-04T17:45:33.554243Z","shell.execute_reply.started":"2022-05-04T17:42:31.369777Z","shell.execute_reply":"2022-05-04T17:45:33.553476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Function to generate sequences","metadata":{}},{"cell_type":"code","source":"def generate(\n    model,\n    tokenizer,\n    prompt,\n    entry_count=1,\n    entry_length=500, #maximum number of words\n    top_p=0.8,\n    temperature=1.,\n):\n    model.eval()\n    generated_num = 0\n    generated_list = []\n\n    filter_value = -float(\"Inf\")\n\n    with torch.no_grad():\n\n        for entry_idx in trange(entry_count):\n\n            entry_finished = False\n            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n\n            for i in range(entry_length):\n                outputs = model(generated, labels=generated)\n                loss, logits = outputs[:2]\n                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n\n                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n                sorted_indices_to_remove = cumulative_probs > top_p\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n                    ..., :-1\n                ].clone()\n                sorted_indices_to_remove[..., 0] = 0\n\n                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n                logits[:, indices_to_remove] = filter_value\n\n                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n                generated = torch.cat((generated, next_token), dim=1)\n\n                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n                    entry_finished = True\n\n                if entry_finished:\n\n                    generated_num = generated_num + 1\n\n                    output_list = list(generated.squeeze().numpy())\n                    output_text = tokenizer.decode(output_list)\n                    generated_list.append(output_text)\n                    break\n            \n            if not entry_finished:\n              output_list = list(generated.squeeze().numpy())\n              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n              generated_list.append(output_text)\n                \n    return generated_list","metadata":{"execution":{"iopub.status.busy":"2022-05-04T18:29:06.122829Z","iopub.execute_input":"2022-05-04T18:29:06.12327Z","iopub.status.idle":"2022-05-04T18:29:06.135972Z","shell.execute_reply.started":"2022-05-04T18:29:06.123234Z","shell.execute_reply":"2022-05-04T18:29:06.1352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Function to generate multiple sentences. Test data should be a dataframe\ndef text_generation(test_data):\n  generated_lyrics = []\n  #for i in range(len(test_data)):\n  x = generate(model.to('cpu'), tokenizer, test_data, entry_count=1)\n  generated_lyrics.append(x)\n  return generated_lyrics","metadata":{"execution":{"iopub.status.busy":"2022-05-04T18:29:11.596697Z","iopub.execute_input":"2022-05-04T18:29:11.59737Z","iopub.status.idle":"2022-05-04T18:29:11.60245Z","shell.execute_reply.started":"2022-05-04T18:29:11.597328Z","shell.execute_reply":"2022-05-04T18:29:11.601757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generating words after \"I Love Deep Learning\"","metadata":{}},{"cell_type":"code","source":"#Run the functions to generate the lyrics\ngenerated_lyrics = text_generation(\"I love Deep Learning\")","metadata":{"execution":{"iopub.status.busy":"2022-05-04T18:29:13.870222Z","iopub.execute_input":"2022-05-04T18:29:13.870683Z","iopub.status.idle":"2022-05-04T18:35:43.128253Z","shell.execute_reply.started":"2022-05-04T18:29:13.870647Z","shell.execute_reply":"2022-05-04T18:35:43.126478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generating texts","metadata":{}},{"cell_type":"code","source":"my_generations=[]\n\nfor i in range(len(generated_lyrics)):\n  a = test_set['Lyric'][i].split()[:] #Get the matching string we want (30 words)\n  b = ' '.join(a)\n  c = ' '.join(generated_lyrics[i]) #Get all that comes after the matching string\n  my_generations.append(c.split(b)[-1])","metadata":{"execution":{"iopub.status.busy":"2022-05-04T18:38:33.27362Z","iopub.execute_input":"2022-05-04T18:38:33.273886Z","iopub.status.idle":"2022-05-04T18:38:33.28291Z","shell.execute_reply.started":"2022-05-04T18:38:33.273858Z","shell.execute_reply":"2022-05-04T18:38:33.281953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" my_generations","metadata":{"execution":{"iopub.status.busy":"2022-05-04T18:38:45.580402Z","iopub.execute_input":"2022-05-04T18:38:45.581081Z","iopub.status.idle":"2022-05-04T18:38:45.589581Z","shell.execute_reply.started":"2022-05-04T18:38:45.581044Z","shell.execute_reply":"2022-05-04T18:38:45.588732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Printing the generated text in a nice format","metadata":{}},{"cell_type":"code","source":"for i in range (len(my_generations[0])-15):\n    print(my_generations[0][i], end=\"\")","metadata":{"execution":{"iopub.status.busy":"2022-05-04T18:39:01.152963Z","iopub.execute_input":"2022-05-04T18:39:01.15352Z","iopub.status.idle":"2022-05-04T18:39:01.379539Z","shell.execute_reply.started":"2022-05-04T18:39:01.15348Z","shell.execute_reply":"2022-05-04T18:39:01.378969Z"},"trusted":true},"execution_count":null,"outputs":[]}]}